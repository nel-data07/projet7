{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52418d18-d703-4c91-a27b-5489e6b63069",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <span style=\"font-size: xx-large; font-weight: bold; color: red;\">\n",
    "        PROJET 7 :\n",
    "        Implementez un modèle de scoring\n",
    "    </span>\n",
    "</p>  \n",
    "\n",
    "__Sommaire__\n",
    "\n",
    "**Partie 1 : Mise en place environnement  MLFlow**\n",
    "- <a href=\"#C0\">Import des bibliotheques</a>\n",
    "- <a href=\"#C1\">Environnement  MLFlow</a>\n",
    "\n",
    "**Partie 2 : Exploration des données**\n",
    "- <a href=\"#C2\">Fonctions utiles</a>\n",
    "- <a href=\"#C3\">Chargement des fichiers</a>\n",
    "- <a>Prétraitements des fichiers</a>\n",
    "    - <a href=\"#C4\"> Test et validation</a>\n",
    "    - <a href=\"#C5\"> bureau et bureau_balance</a>\n",
    "    - <a href=\"#C6\"> previous_application</a>\n",
    "    - <a href=\"#C7\"> posh_cash_balance</a>\n",
    "    - <a href=\"#C8\"> installments_payement</a>\n",
    "    - <a href=\"#C9\"> credit_card_balance</a>\n",
    "- <a>Feature engineering (conservation des features qui ont moins de 80% de données manquantes)</a>\n",
    "    - <a href=\"#C10\"> df </a>\n",
    "    - <a href=\"#C11\"> bureau_agg</a>\n",
    "    - <a href=\"#C12\">previous_applications_agg </a>\n",
    "    - <a href=\"#C13\">installments_payments_agg </a>\n",
    "    - <a href=\"#C14\">credit_card_balance_agg</a>\n",
    "    - <a href=\"#C15\">home_credit </a>\n",
    "- <a>Exploration variable cible</a>\n",
    "    - <a href=\"#C16\">feature target </a>\n",
    "\n",
    "**Partie 3 : Préparation des données**   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600f2e5-0353-44dd-b1d7-9af54017df11",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 1 : Mise en place environnement  MLFlow</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df1914-9865-483e-b280-e1300f4f590c",
   "metadata": {},
   "source": [
    "# <a name=\"C0\"><span style=\"text-decoration: underline;\">Import des bibliothèques</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "1e1df803-f0fc-4ec3-8c1c-cb5f72916f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#pip install mlflow\n",
    "#pip install dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7b3db7-4b2e-40ab-81e5-99ae99450372",
   "metadata": {},
   "source": [
    "# <a name=\"C1\"><span style=\"text-decoration: underline;\">Environnement  MLFlow</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "b018a4e8-43b2-4b38-89f6-917fea46fa15",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[597], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_tracking_uri(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://127.0.0.1:5000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Spécifiez une expérience MLflow\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCredit Scoring Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/tracking/fluent.py:143\u001b[0m, in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (experiment_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    136\u001b[0m     experiment_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    139\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify exactly one of: `experiment_id` or `experiment_name`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    140\u001b[0m         error_code\u001b[38;5;241m=\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experiment_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     experiment \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_experiment_by_name(experiment_name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/tracking/client.py:133\u001b[0m, in \u001b[0;36mMlflowClient.__init__\u001b[0;34m(self, tracking_uri, registry_uri)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    tracking_uri: Address of local or remote tracking server. If not provided, defaults\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m        no such service was set, defaults to the tracking uri of the client.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m final_tracking_uri \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39m_resolve_tracking_uri(tracking_uri)\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_uri \u001b[38;5;241m=\u001b[39m registry_utils\u001b[38;5;241m.\u001b[39m_resolve_registry_uri(registry_uri, tracking_uri)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracking_client \u001b[38;5;241m=\u001b[39m TrackingServiceClient(final_tracking_uri)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/tracking/_model_registry/utils.py:131\u001b[0m, in \u001b[0;36m_resolve_registry_uri\u001b[0;34m(registry_uri, tracking_uri)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resolve_registry_uri\u001b[39m(registry_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tracking_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m registry_uri \u001b[38;5;129;01mor\u001b[39;00m _get_registry_uri_from_context() \u001b[38;5;129;01mor\u001b[39;00m _resolve_tracking_uri(tracking_uri)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/tracking/_model_registry/utils.py:97\u001b[0m, in \u001b[0;36m_get_registry_uri_from_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _registry_uri \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _registry_uri\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (uri \u001b[38;5;241m:=\u001b[39m MLFLOW_REGISTRY_URI\u001b[38;5;241m.\u001b[39mget()) \u001b[38;5;129;01mor\u001b[39;00m (uri \u001b[38;5;241m:=\u001b[39m _get_registry_uri_from_spark_session()):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uri\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _registry_uri\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/tracking/_model_registry/utils.py:82\u001b[0m, in \u001b[0;36m_get_registry_uri_from_spark_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_registry_uri_from_spark_session\u001b[39m():\n\u001b[0;32m---> 82\u001b[0m     session \u001b[38;5;241m=\u001b[39m _get_active_spark_session()\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/mlflow/utils/_spark_utils.py:11\u001b[0m, in \u001b[0;36m_get_active_spark_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_active_spark_session\u001b[39m():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Return None if user doesn't have PySpark installed\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1138\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1078\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1473\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1431\u001b[0m, in \u001b[0;36m_path_importer_cache\u001b[0;34m(cls, path)\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "# Configuration du serveur de suivi MLflow (ici en local)\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Spécifiez une expérience MLflow\n",
    "mlflow.set_experiment(\"Credit Scoring Model\")\n",
    "\n",
    "#lancer dans le terminal pour visualiser : mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46bf6-2f90-4b61-87db-1793249b9747",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 2 : Analyse exploratoire des données</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f50e32-c508-4c88-a80a-5abe1de0987e",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Fonctions utiles</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05a219-ab49-485a-a156-6a19ea16ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431abd76-a5ec-4ec7-af79-abcf460259c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c5f3-175f-4974-a844-c2e422956fdd",
   "metadata": {},
   "source": [
    "# <a name=\"C3\"><span style=\"text-decoration: underline;\">Chargement des fichiers</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07de4f-d46b-49fa-bba8-4861bdf31ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers en utilisant dask\n",
    "sample_submission = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/sample_submission.csv\")\n",
    "home_credit = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/HomeCredit_columns_description.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dc1cb-45fc-46e0-b049-8b1ad4fb967b",
   "metadata": {},
   "source": [
    "# <a name=\"C4\"><span style=\"text-decoration: underline;\">Prétraitement des fichiers de validation et d'entrainement</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4b176-3da0-43d8-abaa-65cd8d5962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des fichiers application_train.csv et application_test.csv sans limitation de lignes\n",
    "def application_train_test(nan_as_category=False):\n",
    "    # Lire les données complètes et fusionner\n",
    "    df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_train.csv\")\n",
    "    test_df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_test.csv\")\n",
    "    print(\"Échantillons d'entraînement : {}, échantillons de test : {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index(drop=True)\n",
    "    \n",
    "    # Optionnel : Supprimer les applications avec CODE_GENDER 'XNA' (ensemble d'entraînement)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Caractéristiques catégorielles avec encodage binaire (0 ou 1 ; deux catégories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "        \n",
    "    # Caractéristiques catégorielles avec encodage One-Hot\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # Valeurs NaN pour DAYS_EMPLOYED : 365.243 -> NaN\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Quelques nouvelles caractéristiques simples (pourcentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463a0dd-83b7-406d-9457-ad83a463f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction pour voir le résultat\n",
    "df = application_train_test(nan_as_category=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afb9cc-c5b5-4579-af44-79815f83adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training/test data shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe9ebc-e0e7-47a7-a108-809507048b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(df)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffb7e9-80d6-4458-b932-93653f8a0af9",
   "metadata": {},
   "source": [
    "Dans des travaux ultérieurs, nous utiliserons des modèles tels que XGBoost qui peuvent gérer les valeurs manquantes sans avoir besoin d’imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20add28a-8b16-4256-8048-1925a19ae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre des differents types\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445230e0-95f1-4aef-add8-79f7511943e4",
   "metadata": {},
   "source": [
    "# <a name=\"C5\"><span style=\"text-decoration: underline;\">Prétraitement des données bureau et bureau_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fbb68-b107-4d60-b4d5-acecf4ef902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des fichiers bureau.csv et bureau_balance.csv\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/bureau.csv\")\n",
    "    bb = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/bureau_balance.csv\")\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance : Effectuer des agrégations et fusionner avec bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Caractéristiques numériques de bureau et bureau_balance\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Caractéristiques catégorielles de bureau et bureau_balance\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau : Crédits actifs - en utilisant uniquement les agrégations numériques\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau : Crédits clôturés - en utilisant uniquement les agrégations numériques\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0ef24-9601-4370-b99c-2c0935eac1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et afficher le DataFrame résultant\n",
    "bureau_agg = bureau_and_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf4acf-2e8b-417d-bb83-afed9fc05864",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd8e75-3d20-4add-a124-2223300a0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bureau/bureau_balance : ', bureau_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e91d4-c304-416f-8bbe-e535a3977583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(bureau_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c34632-46bc-4272-8e48-892347ef21d7",
   "metadata": {},
   "source": [
    "# <a name=\"C5\"><span style=\"text-decoration: underline;\">Prétraitement des données de previous_application</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65519a9d-f9b8-42f6-9b66-4dab9ab7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'encodage One-Hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    df = pd.get_dummies(df, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Prétraitement de previous_applications.csv\n",
    "def previous_applications(nan_as_category=True):\n",
    "    prev = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/previous_application.csv\")\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "    \n",
    "    # Valeurs de 365.243 jours -> NaN\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Ajouter une caractéristique : pourcentage de la valeur demandée / valeur reçue\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    # Caractéristiques numériques des demandes précédentes\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Caractéristiques catégorielles des demandes précédentes\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Demandes précédentes : Demandes approuvées - uniquement les caractéristiques numériques\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Demandes précédentes : Demandes refusées - uniquement les caractéristiques numériques\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1159ab4-e9e7-447f-ab11-4fa2c2e74ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter la fonction et afficher le DataFrame\n",
    "previous_applications_agg = previous_applications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b89cdc-cc94-4cbb-a21d-6e1c8ee56190",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_applications_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e5699-f11b-4f0c-9b59-1223fa4d7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('previous : ', previous_applications_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f381e-2f93-4b07-9dab-ff06154f5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(previous_applications_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5847a97-b515-4ae3-b23d-0f2062c49670",
   "metadata": {},
   "source": [
    "# <a name=\"C6\"><span style=\"text-decoration: underline;\">Prétraitement des données de POS_CASH_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8b437-9a3e-444f-a1de-8588fff9799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement de POS_CASH_balance.csv\n",
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/POS_CASH_balance.csv\")\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "    \n",
    "    # Caractéristiques\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de comptes POS cash\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250bc92-cf13-4620-b456-b7da581d68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "pos_cash_agg = pos_cash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856d667-45e1-469b-ae5d-ab7b893306df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50972410-4c32-4f3c-bfb0-af437b04440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pos_cash : ', pos_cash_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c5008-f395-4991-ba10-24e2d4fc8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(pos_cash_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f063cc-0b88-41b7-809f-a3f5a808a03d",
   "metadata": {},
   "source": [
    "# <a name=\"C7\"><span style=\"text-decoration: underline;\">Prétraitement des données installments_payments</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e255d-eecd-4f58-b0bc-85eeab0e922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'encodage One-Hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    df = pd.get_dummies(df, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Prétraitement de installments_payments.csv\n",
    "def installments_payments(nan_as_category=True):\n",
    "    # Lire les données de installments_payments\n",
    "    ins = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/installments_payments.csv\")\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "    \n",
    "    # Pourcentage et différence payés dans chaque versement (montant payé et valeur du versement)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    \n",
    "    # Jours de retard et jours avant échéance (pas de valeurs négatives)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # Caractéristiques : effectuer des agrégations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de comptes de versements\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Libérer la mémoire\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    \n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5fc28-1e29-4005-acdf-e4868c24be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "installments_payments_agg = installments_payments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8b0f4-5dc9-4095-af7d-995c3b02a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb20c2d-65a2-4900-af33-609ed29ed3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('installments payments : ', installments_payments_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76797df-81dc-44bb-8448-3bc9a1898a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(installments_payments_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afc620-c8bd-47f3-91f9-9254d7223c0d",
   "metadata": {},
   "source": [
    "# <a name=\"C7\"><span style=\"text-decoration: underline;\">Prétraitement des données credit_card_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd6dc0-0ad4-4b1c-8fa1-73d00ee9688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement de credit_card_balance.csv\n",
    "def credit_card_balance(nan_as_category=True):\n",
    "    cc = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/credit_card_balance.csv\")\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "    \n",
    "    # Agrégations générales\n",
    "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de lignes de carte de crédit\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c63f54-da83-4e49-ac08-2e28c0a3e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "credit_card_balance_agg = credit_card_balance()\n",
    "credit_card_balance_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cca21-88d1-4205-8e63-2326dbb6e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('credit_card_balance_agg : ', credit_card_balance_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22577e1b-f47a-47d7-99f4-ed8a49326ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(credit_card_balance_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e16ada-d572-4977-bf0b-0e8d6a5b2e00",
   "metadata": {},
   "source": [
    "**sample_submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85702fa-6bc6-4c00-9bc9-6d96bf3fee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sample_submission : ', sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29894bd3-ed75-4542-92d8-f5e4860fe218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(sample_submission)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c157216-fca2-498d-9d86-bb14ad109880",
   "metadata": {},
   "source": [
    "**HomeCredit_columns_description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc1e38-4554-42cf-bb4e-9d0c3e5645fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home_credit : ', home_credit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342ff7b-c309-479a-a2ef-9a7eb4dd2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(home_credit)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4550f13-6fce-4e33-9490-175e062dc9e6",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes df</span></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58892a8-7a86-4ff5-af1e-13134fec5f06",
   "metadata": {},
   "source": [
    "les data frames qui ont des données manquantes : \n",
    "- df\n",
    "- bureau_agg\n",
    "- previous_applications_agg\n",
    "- installments_payments_agg\n",
    "- credit_card_balance_agg\n",
    "- home_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbe9b8-174d-41a6-bc48-64cb5bacd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(df)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbde27-f775-47ed-879c-2e0285fff27a",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes bureau_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992166c-a3ca-4f71-a8be-832c646a062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(bureau_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59526c3b-30a7-4f6e-b314-374e5d77fd3a",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes previous_applications_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85d40f-d957-446c-9f59-8153abb1e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(previous_applications_agg)\n",
    "\n",
    "# Filtrer pour obtenir les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "features_to_drop = missing_values[missing_values['% of Total Values'] > 80].index\n",
    "\n",
    "# Supprimer les colonnes avec plus de 80 % de valeurs manquantes du DataFrame principal\n",
    "previous_applications_agg_cleaned = previous_applications_agg.drop(columns=features_to_drop)\n",
    "\n",
    "# Afficher la forme du DataFrame après la suppression\n",
    "print(\"DataFrame initial :\", previous_applications_agg.shape)\n",
    "print(\"DataFrame après suppression des colonnes avec plus de 80 % de valeurs manquantes :\", previous_applications_agg_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846978cf-2f9b-43f1-8845-fbf0a58da77e",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes installments_payments_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d56b6-be5b-4813-a26e-b4a1c4df8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(installments_payments_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9699d7-725c-4a5a-976c-cefef1d71d5d",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes credit_card_balance_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedea3b-f17b-4bf0-bfc2-b32699395e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(credit_card_balance_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20340ba4-8ac1-4984-a832-24c0396e612e",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données manquantes home_credit</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470b079-8ad9-47e3-967e-f8f2600edbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(home_credit)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27d387-59e6-450a-87e8-23cc9e115a3c",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Exploration variable cible (target)</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c076c12-066f-4712-a1a0-b0718683634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a4bf6-4b20-4bc5-b59c-2a4dba6dd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anomalous flag column\n",
    "df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "df['DAYS_EMPLOYED'].plot.hist(title = 'Histogramme de l’emploi par jour');\n",
    "plt.xlabel('Jours d’emploi');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48177cc-585e-4a65-8ef3-f515fdf3b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les corrélations avec la cible et trier\n",
    "correlations = df.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les corrélations\n",
    "print('Corrélations les plus positives :\\n', correlations.tail(15))\n",
    "print('\\nCorrélations les plus négatives :\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf9bbc-dccb-448a-97cd-ab3d3a20f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations d'âge dans un DataFrame séparé\n",
    "age_data = df[['TARGET', 'DAYS_BIRTH']]\n",
    "\n",
    "# Prendre la valeur absolue de DAYS_BIRTH pour enlever les valeurs négatives, puis convertir en années\n",
    "age_data['DAYS_BIRTH'] = age_data['DAYS_BIRTH'].abs()  # Cette ligne prend la valeur absolue\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Créer des intervalles d'âge personnalisés de 5 en 5 ans\n",
    "bins = [20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=bins)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(age_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc64b9-cb0f-4b63-a05c-66a871651b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb58ba-76fb-44f5-850e-69f7794cdef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graphique des tranches d'âge et de la moyenne de la cible sous forme de diagramme à barres\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Étiquettes du graphique\n",
    "plt.xticks(rotation = 75)\n",
    "plt.xlabel('Groupe d\\'âge (années)')\n",
    "plt.ylabel('Défaut de remboursement (%)')\n",
    "plt.title('Défaut de remboursement par groupe d\\'âge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2e564-1ef0-4575-a812-ce6745028159",
   "metadata": {},
   "source": [
    "Il y a une tendance claire : les jeunes demandeurs sont plus susceptibles de ne pas rembourser le prêt! Le taux de non-remboursement est supérieur à 10 % pour les trois groupes d’âge les plus jeunes et inférieur à 5 % pour le groupe d’âge le plus âgé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf27a4-b790-4374-b0a6-1d2cb5ea648c",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 3 : Préparation des données pour la modélisation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ba6b6-78f7-4795-8d28-5eb374f4c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc75f4-849a-4405-a9ed-2c09f99db955",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Prétraitement des données</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e164e4-211d-4a9f-91a2-e7db57a0e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_train.csv\")\n",
    "\n",
    "# Fonction d'encodage one-hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Appliquer l'encodage one-hot\n",
    "df, new_columns = one_hot_encoder(df, nan_as_category=True)\n",
    "\n",
    "# Vérifier les colonnes disponibles\n",
    "print(\"Colonnes disponibles :\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4d551-0be5-41c3-be91-8475c67ad33d",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Traitement données train avec/sans réequilibrage</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af873a-d7f0-4279-b362-0e8d8c05a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : DataFrame sans rééquilibrage\n",
    "df_no_balancing = df.copy()\n",
    "\n",
    "# Vérifier les proportions dans le jeu déséquilibré\n",
    "print(\"Proportions sans rééquilibrage :\")\n",
    "print(df_no_balancing['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5b785-cb70-4a88-ac63-31e29701bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données pour ADASYN\n",
    "X = df.drop(columns=['TARGET'])  # Features\n",
    "y = df['TARGET']  # Cible\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "print(\"Nombre de valeurs manquantes avant imputation :\", X.isnull().sum().sum())\n",
    "\n",
    "# Imputer les valeurs manquantes\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Vérifier les proportions actuelles\n",
    "print(\"Proportions avant rééquilibrage :\", Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f9db3-9373-4ff0-8f12-fa0c7ab4b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le nombre exact d'échantillons pour équilibrer\n",
    "count_majority = y.value_counts()[0]  # Nombre dans la classe majoritaire\n",
    "count_minority = y.value_counts()[1]  # Nombre dans la classe minoritaire\n",
    "samples_needed = count_majority - count_minority  # Nombre d'échantillons nécessaires pour équilibrer\n",
    "\n",
    "print(f\"Classe majoritaire : {count_majority}\")\n",
    "print(f\"Classe minoritaire : {count_minority}\")\n",
    "print(f\"Échantillons nécessaires pour équilibrer : {samples_needed}\")\n",
    "\n",
    "# Rééquilibrer les données avec ADASYN\n",
    "adasyn = ADASYN(sampling_strategy={1: count_majority}, random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b82cf1-ef7b-4f08-9aac-6a3854941e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le DataFrame rééquilibré après ADASYN\n",
    "df_with_balancing = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_with_balancing['TARGET'] = y_resampled\n",
    "\n",
    "# Séparer les classes majoritaire et minoritaire\n",
    "df_majority = df_with_balancing[df_with_balancing['TARGET'] == 0]\n",
    "df_minority = df_with_balancing[df_with_balancing['TARGET'] == 1]\n",
    "\n",
    "# Sous-échantillonnage de la classe minoritaire\n",
    "df_minority_balanced = resample(df_minority, \n",
    "                                replace=False,  # Pas de duplication\n",
    "                                n_samples=len(df_majority),  # Taille cible = classe majoritaire\n",
    "                                random_state=42)\n",
    "\n",
    "# Fusionner les deux classes pour obtenir un DataFrame équilibré\n",
    "df_balanced = pd.concat([df_majority, df_minority_balanced])\n",
    "\n",
    "# Vérifier les proportions finales\n",
    "print(\"Proportions finales après ajustement exact :\")\n",
    "print(df_balanced['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b12ed7-d4d6-43db-9e56-b320d0309710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde ou utilisation des deux DataFrames\n",
    "df_no_balancing.to_csv(\"/Users/Nelly/Desktop/projet 7/data/dataset_sans_reequilibrage.csv\", index=False)\n",
    "df_balanced.to_csv(\"/Users/Nelly/Desktop/projet 7/data/dataset_avec_reequilibrage.csv\", index=False)\n",
    "\n",
    "print(\"Les fichiers ont été sauvegardés avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f150a-8635-4871-80bc-a69b4fb34fe1",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Modélisation</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e3539-66bb-4b23-a1f2-541834e4c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd7fea-6f51-409b-9953-bc60b7f0c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4076f-c68e-4d54-9494-0a1cecd8b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les deux fichiers\n",
    "df_no_balancing = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/dataset_sans_reequilibrage.csv\")\n",
    "df_balanced = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/dataset_avec_reequilibrage.csv\")\n",
    "\n",
    "# Préparer les features (X) et la cible (y)\n",
    "def prepare_data(df):\n",
    "    X = df.drop(columns=['TARGET'])\n",
    "    y = df['TARGET']\n",
    "    return X, y\n",
    "\n",
    "X_no_balancing, y_no_balancing = prepare_data(df_no_balancing)\n",
    "X_balanced, y_balanced = prepare_data(df_balanced)\n",
    "\n",
    "# Diviser les jeux de données en entraînement et test\n",
    "X_train_no, X_test_no, y_train_no, y_test_no = train_test_split(X_no_balancing, y_no_balancing, test_size=0.3, random_state=42, stratify=y_no_balancing)\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5309e-4ce4-430e-a277-5c8d213862cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs manquantes pour chaque dataset\n",
    "missing_no = X_no_balancing.isnull().sum()  # Somme des valeurs manquantes par colonne\n",
    "missing_bal = X_balanced.isnull().sum()    # Somme des valeurs manquantes par colonne\n",
    "\n",
    "# Proportion des données manquantes en pourcentage\n",
    "missing_no_percent = (missing_no / len(X_no_balancing)) * 100\n",
    "missing_bal_percent = (missing_bal / len(X_balanced)) * 100\n",
    "\n",
    "print(\"Pourcentage des données manquantes (sans rééquilibrage) :\")\n",
    "print(missing_no_percent[missing_no_percent > 0])\n",
    "\n",
    "print(\"\\nPourcentage des données manquantes (avec rééquilibrage) :\")\n",
    "print(missing_bal_percent[missing_bal_percent > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdfd53-a72d-409e-8e83-2e4a2d6abef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualisation des colonnes avec données manquantes\n",
    "def plot_missing_data(missing_data, title):\n",
    "    filtered_data = missing_data[missing_data > 0]\n",
    "    if filtered_data.empty:\n",
    "        print(f\"Aucune donnée manquante pour {title}.\")\n",
    "        return\n",
    "    filtered_data.sort_values(ascending=False).plot(\n",
    "        kind='bar', figsize=(12, 6), color=\"skyblue\"\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Pourcentage de données manquantes\")\n",
    "    plt.show()\n",
    "\n",
    "# Utilisation de la fonction corrigée\n",
    "print(\"Graphique des données manquantes (sans rééquilibrage) :\")\n",
    "plot_missing_data(missing_no_percent, \"Données manquantes (sans rééquilibrage)\")\n",
    "\n",
    "print(\"Graphique des données manquantes (avec rééquilibrage) :\")\n",
    "plot_missing_data(missing_bal_percent, \"Données manquantes (avec rééquilibrage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37335948-1f70-4140-958d-d5a6b2fd5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes avec plus de 50% de données manquantes\n",
    "high_missing_cols = missing_no_percent[missing_no_percent > 50].index\n",
    "X_no_balancing = X_no_balancing.drop(columns=high_missing_cols)\n",
    "X_balanced = X_balanced.drop(columns=high_missing_cols)\n",
    "\n",
    "# Supprimer les colonnes complètement vides\n",
    "X_no_balancing = X_no_balancing.dropna(axis=1, how='all')\n",
    "X_balanced = X_balanced.dropna(axis=1, how='all')\n",
    "\n",
    "# Vérification des données manquantes\n",
    "print(\"Données manquantes dans X_no_balancing :\", X_no_balancing.isnull().sum().sum())\n",
    "print(\"Données manquantes dans X_balanced :\", X_balanced.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe0372-fda4-4eb8-a92f-97cf660417a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55267f4-6b6f-4be7-9cd3-b5e109abfc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fonction pour calculer le coût métier\n",
    "def calculate_business_cost(y_true, y_pred, fn_cost=10, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Calcule le coût métier basé sur les faux négatifs (FN) et faux positifs (FP).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    business_cost = fn * fn_cost + fp * fp_cost\n",
    "    return business_cost\n",
    "\n",
    "# Diviser les données sans rééquilibrage\n",
    "X_train_no, X_test_no, y_train_no, y_test_no = train_test_split(\n",
    "    X_no_balancing, y_no_balancing, test_size=0.3, random_state=42, stratify=y_no_balancing\n",
    ")\n",
    "\n",
    "# Diviser les données avec rééquilibrage\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.3, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Définir les jeux de données\n",
    "datasets = {\n",
    "    \"Sans rééquilibrage\": (X_train_no, X_test_no, y_train_no, y_test_no),\n",
    "    \"Avec rééquilibrage\": (X_train_bal, X_test_bal, y_train_bal, y_test_bal)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e08784-8401-4a02-a023-b3a42c2f7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Configurez MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Credit Scoring Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a680f-508d-4c9f-a9d4-227ff671176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def objective(params, model_type, X_train, X_test, y_train, y_test):\n",
    "    print(\"Début de la fonction objective\")\n",
    "\n",
    "    # Démarrer un run MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_type} optimization\"):\n",
    "        print(\"Run MLflow démarré\")\n",
    "        active_run = mlflow.active_run()\n",
    "        if active_run:\n",
    "            print(f\"Run ID actif : {active_run.info.run_id}\")\n",
    "        else:\n",
    "            print(\"Erreur : Aucun run actif détecté !\")\n",
    "\n",
    "        # Nettoyage des colonnes\n",
    "        print(\"Nettoyage des colonnes...\")\n",
    "        X_train = X_train.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "        X_test = X_test.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "\n",
    "        # Initialisation du modèle\n",
    "        print(f\"Initialisation du modèle {model_type}...\")\n",
    "        if model_type == \"LightGBM\":\n",
    "            model = lgb.LGBMClassifier(\n",
    "                n_estimators=int(params.get('n_estimators', 100)),\n",
    "                num_leaves=int(params.get('num_leaves', 31)),\n",
    "                learning_rate=params.get('learning_rate', 0.1),\n",
    "                max_depth=int(params.get('max_depth', -1)),\n",
    "                subsample=params.get('subsample', 1.0),\n",
    "                colsample_bytree=params.get('colsample_bytree', 1.0),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif model_type == \"RandomForest\":\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=int(params.get('n_estimators', 100)),\n",
    "                max_depth=int(params.get('max_depth', None)),\n",
    "                min_samples_split=params.get('min_samples_split', 2),\n",
    "                min_samples_leaf=params.get('min_samples_leaf', 1),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif model_type == \"LogisticRegression\":\n",
    "            model = LogisticRegression(\n",
    "                C=params.get('C', 1.0),\n",
    "                solver=params.get('solver', 'lbfgs'),\n",
    "                max_iter=1000,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Type de modèle non supporté : {}\".format(model_type))\n",
    "\n",
    "        print(\"Entraînement du modèle...\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Calcul des métriques...\")\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Calcul des métriques pertinentes\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # Calcul du coût métier\n",
    "        business_cost = calculate_business_cost(y_test, y_pred)\n",
    "\n",
    "        # Log des métriques dans MLflow\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"Precision\", precision)\n",
    "        mlflow.log_metric(\"Recall\", recall)\n",
    "        mlflow.log_metric(\"F1-Score\", f1)\n",
    "        mlflow.log_metric(\"Business Cost\", business_cost)\n",
    "\n",
    "        print(f\"Métriques loguées : AUC={auc}, Accuracy={accuracy}, Precision={precision}, Recall={recall}, F1-Score={f1}, Business Cost={business_cost}\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, f\"{model_type}_Model\")\n",
    "\n",
    "        print(f\"Run ID final pour le Model Registry : {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "        return {\n",
    "            'loss': -auc,  # AUC négatif pour minimisation\n",
    "            'status': STATUS_OK,\n",
    "            'auc': auc,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'business_cost': business_cost\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02267b97-8de8-4742-a260-d5639003f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace des hyperparamètres\n",
    "param_space_lgb = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 50, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'max_depth': hp.quniform('max_depth', -1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bba88-afd9-4248-9c2a-77dd847f02dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Définir les jeux de données\n",
    "datasets = {\n",
    "    \"Sans rééquilibrage\": (X_train_no, X_test_no, y_train_no, y_test_no),\n",
    "    \"Avec rééquilibrage\": (X_train_bal, X_test_bal, y_train_bal, y_test_bal)\n",
    "}\n",
    "\n",
    "# Tester chaque jeu de données\n",
    "for name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"Optimisation pour le dataset : {name}\")\n",
    "    \n",
    "    trials_lgb = Trials()\n",
    "    best_params_lgb = fmin(\n",
    "        fn=lambda params: objective(params, \"LightGBM\", X_train, X_test, y_train, y_test),\n",
    "        space=param_space_lgb,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=50,\n",
    "        trials=trials_lgb\n",
    "    )\n",
    "    \n",
    "    print(f\"Meilleurs hyperparamètres pour {name} :\", best_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b4401-3e73-42cf-9770-8e37dd518c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres pour Random Forest\n",
    "print(\"Optimisation des hyperparamètres pour Random Forest...\")\n",
    "\n",
    "param_space_rf = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 30, 1),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5)\n",
    "}\n",
    "\n",
    "trials_rf = Trials()\n",
    "best_params_rf = fmin(\n",
    "    fn=lambda params: objective(params, \"RandomForest\", X_train, X_test, y_train, y_test),\n",
    "    space=param_space_rf,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials_rf\n",
    ")\n",
    "\n",
    "print(\"Meilleurs hyperparamètres pour Random Forest :\", best_params_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae1b38-953d-4f45-ada4-a0980ef4519b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres pour Logistic Regression\n",
    "print(\"Optimisation des hyperparamètres pour Logistic Regression...\")\n",
    "\n",
    "param_space_lr = {\n",
    "    'C': hp.loguniform('C', -4, 2),  # Exponentielle pour couvrir une large plage\n",
    "    'solver': hp.choice('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "}\n",
    "\n",
    "trials_lr = Trials()\n",
    "best_params_lr = fmin(\n",
    "    fn=lambda params: objective(params, \"LogisticRegression\", X_train, X_test, y_train, y_test),\n",
    "    space=param_space_lr,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials_lr\n",
    ")\n",
    "\n",
    "print(\"Meilleurs hyperparamètres pour Logistic Regression :\", best_params_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2038e-df33-458f-a2f9-95bf342a0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4b885-e5d9-4a97-ba34-8b26b6a820bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# Fonction pour évaluer un modèle optimisé\n",
    "def evaluate_optimized_model(model, X_train, X_test, y_train, y_test, fn_cost=10, fp_cost=1):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_cost = float('inf')\n",
    "    results_by_threshold = []\n",
    "\n",
    "    thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        cost = calculate_business_cost(y_test, y_pred, fn_cost, fp_cost)\n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "            best_threshold = threshold\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        results_by_threshold.append({\n",
    "            \"Threshold\": threshold,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1,\n",
    "            \"Business Cost\": cost\n",
    "        })\n",
    "\n",
    "    final_metrics = {\n",
    "        \"Optimal Threshold\": best_threshold,\n",
    "        \"Optimal Business Cost\": best_cost,\n",
    "        \"Results by Threshold\": results_by_threshold\n",
    "    }\n",
    "    return final_metrics\n",
    "\n",
    "# Imputation des données pour gérer les NaN\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_no = imputer.fit_transform(X_train_no)\n",
    "X_test_no = imputer.transform(X_test_no)\n",
    "\n",
    "X_train_bal = imputer.fit_transform(X_train_bal)\n",
    "X_test_bal = imputer.transform(X_test_bal)\n",
    "\n",
    "# Mise à l'échelle des données\n",
    "scaler = StandardScaler()\n",
    "X_train_no_scaled = scaler.fit_transform(X_train_no)\n",
    "X_test_no_scaled = scaler.transform(X_test_no)\n",
    "\n",
    "X_train_bal_scaled = scaler.fit_transform(X_train_bal)\n",
    "X_test_bal_scaled = scaler.transform(X_test_bal)\n",
    "\n",
    "# Initialisation des résultats\n",
    "optimized_results_no = {}\n",
    "optimized_results_bal = {}\n",
    "\n",
    "# Évaluation des modèles sans rééquilibrage\n",
    "print(\"Évaluation sur les données SANS rééquilibrage :\")\n",
    "best_model_lgb_no = lgb.LGBMClassifier(\n",
    "    n_estimators=int(best_params_lgb['n_estimators']),\n",
    "    num_leaves=int(best_params_lgb['num_leaves']),\n",
    "    learning_rate=best_params_lgb['learning_rate'],\n",
    "    max_depth=int(best_params_lgb['max_depth']),\n",
    "    subsample=best_params_lgb['subsample'],\n",
    "    colsample_bytree=best_params_lgb['colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"LightGBM\"] = evaluate_optimized_model(best_model_lgb_no, X_train_no, X_test_no, y_train_no, y_test_no)\n",
    "\n",
    "best_model_rf_no = RandomForestClassifier(\n",
    "    n_estimators=int(best_params_rf['n_estimators']),\n",
    "    max_depth=int(best_params_rf['max_depth']),\n",
    "    min_samples_split=best_params_rf['min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"Random Forest\"] = evaluate_optimized_model(best_model_rf_no, X_train_no, X_test_no, y_train_no, y_test_no)\n",
    "\n",
    "best_model_lr_no = LogisticRegression(\n",
    "    C=best_params_lr['C'],\n",
    "    solver=['liblinear', 'lbfgs'][best_params_lr['solver']],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"Logistic Regression\"] = evaluate_optimized_model(\n",
    "    best_model_lr_no, X_train_no_scaled, X_test_no_scaled, y_train_no, y_test_no\n",
    ")\n",
    "\n",
    "# Évaluation des modèles avec rééquilibrage\n",
    "print(\"\\nÉvaluation sur les données AVEC rééquilibrage :\")\n",
    "best_model_lgb_bal = lgb.LGBMClassifier(\n",
    "    n_estimators=int(best_params_lgb['n_estimators']),\n",
    "    num_leaves=int(best_params_lgb['num_leaves']),\n",
    "    learning_rate=best_params_lgb['learning_rate'],\n",
    "    max_depth=int(best_params_lgb['max_depth']),\n",
    "    subsample=best_params_lgb['subsample'],\n",
    "    colsample_bytree=best_params_lgb['colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"LightGBM\"] = evaluate_optimized_model(best_model_lgb_bal, X_train_bal, X_test_bal, y_train_bal, y_test_bal)\n",
    "\n",
    "best_model_rf_bal = RandomForestClassifier(\n",
    "    n_estimators=int(best_params_rf['n_estimators']),\n",
    "    max_depth=int(best_params_rf['max_depth']),\n",
    "    min_samples_split=best_params_rf['min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"Random Forest\"] = evaluate_optimized_model(best_model_rf_bal, X_train_bal, X_test_bal, y_train_bal, y_test_bal)\n",
    "\n",
    "best_model_lr_bal = LogisticRegression(\n",
    "    C=best_params_lr['C'],\n",
    "    solver=['liblinear', 'lbfgs'][best_params_lr['solver']],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"Logistic Regression\"] = evaluate_optimized_model(\n",
    "    best_model_lr_bal, X_train_bal_scaled, X_test_bal_scaled, y_train_bal, y_test_bal\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e14519-66f8-425a-9a9a-b1520c7f321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_results(results, title):\n",
    "    \"\"\"\n",
    "    Affiche les résultats de manière condensée pour chaque modèle, avec des espaces entre les sections.\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    summary_results = []\n",
    "\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\nModèle : {model_name}\")\n",
    "        print(f\"Seuil Optimal : {metrics['Optimal Threshold']:.2f}\")\n",
    "        print(f\"Coût Métier Optimal : {metrics['Optimal Business Cost']:.2f}\\n\")  # Ajout d'un espace ici\n",
    "        \n",
    "        # Résumé au seuil optimal\n",
    "        optimal_metrics = next((res for res in metrics[\"Results by Threshold\"] \n",
    "                                if res[\"Threshold\"] == metrics[\"Optimal Threshold\"]), None)\n",
    "        if optimal_metrics:\n",
    "            summary_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"AUC\": optimal_metrics[\"AUC\"],\n",
    "                \"Accuracy\": optimal_metrics[\"Accuracy\"],\n",
    "                \"Precision\": optimal_metrics[\"Precision\"],\n",
    "                \"Recall\": optimal_metrics[\"Recall\"],\n",
    "                \"F1-Score\": optimal_metrics[\"F1-Score\"],\n",
    "                \"Business Cost\": optimal_metrics[\"Business Cost\"],\n",
    "                \"Optimal Threshold\": optimal_metrics[\"Threshold\"]\n",
    "            })\n",
    "    \n",
    "    # Afficher les résultats sous forme de tableau\n",
    "    results_df = pd.DataFrame(summary_results)\n",
    "    print(results_df.to_string(index=False))  # Le tableau est affiché ici avec un espace au-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3815a-c7c1-48d0-b9fe-9f2671aa35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des résultats\n",
    "display_results(optimized_results_no, \"Résultats pour les modèles optimisés SANS rééquilibrage :\")\n",
    "print(\"\\n\")  # Ajouter un espace pour séparer les deux blocs de résultats\n",
    "display_results(optimized_results_bal, \"Résultats pour les modèles optimisés AVEC rééquilibrage :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7213ff2-695c-42eb-905d-02e6f81167a0",
   "metadata": {},
   "source": [
    "# Analyse des résultats\n",
    "\n",
    "## Modèles optimisés SANS rééquilibrage des données\n",
    "\n",
    "### Modèle : LightGBM\n",
    "- **Seuil Optimal** : 0.09  \n",
    "- **Coût Métier Optimal** : 48,908.00  \n",
    "\n",
    "**Analyse** :  \n",
    "LightGBM montre une performance correcte avec une AUC de 0.759 et une accuracy de 73.2 %. Toutefois, la précision est faible (0.177), ce qui indique un grand nombre de faux positifs. Le rappel (0.639) est acceptable, mais le coût métier optimal reste élevé.\n",
    "\n",
    "### Modèle : Random Forest\n",
    "- **Seuil Optimal** : 0.09  \n",
    "- **Coût Métier Optimal** : 56,132.00  \n",
    "\n",
    "**Analyse** :  \n",
    "Random Forest a des performances légèrement inférieures à celles de LightGBM en termes d'AUC (0.702) et d'accuracy (75.5 %). La précision (0.164) est faible, et le rappel est proche de zéro (0.500), ce qui reflète une incapacité à bien identifier les vrais positifs. Le coût métier est également plus élevé.\n",
    "\n",
    "### Modèle : Logistic Regression\n",
    "- **Seuil Optimal** : 0.09  \n",
    "- **Coût Métier Optimal** : 50,478.00  \n",
    "\n",
    "**Analyse** :  \n",
    "La régression logistique présente une AUC légèrement supérieure (0.744) à Random Forest, mais toujours inférieure à LightGBM. L'accuracy est comparable (72.1 %), mais la précision (0.169) et le rappel (0.630) restent faibles. Le coût métier est un peu plus bas que Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## Modèles optimisés AVEC rééquilibrage des données\n",
    "\n",
    "### Modèle : LightGBM\n",
    "- **Seuil Optimal** : 0.10  \n",
    "- **Coût Métier Optimal** : 48,899.00  \n",
    "\n",
    "**Analyse** :  \n",
    "Après rééquilibrage des données, LightGBM améliore considérablement ses performances : AUC de 0.979 et accuracy de 85.3 %. La précision (0.791) et le rappel (0.968) sont particulièrement élevés, ce qui reflète un excellent compromis entre faux positifs et faux négatifs. Le coût métier est également minimal, ce qui en fait le modèle le plus performant.\n",
    "\n",
    "### Modèle : Random Forest\n",
    "- **Seuil Optimal** : 0.37  \n",
    "- **Coût Métier Optimal** : 75,952.00  \n",
    "\n",
    "**Analyse** :  \n",
    "Le rééquilibrage améliore les performances de Random Forest, mais elles restent inférieures à celles de LightGBM. L'AUC passe à 0.990, mais la précision (0.557) et le rappel (0.989) restent en deçà des attentes. Le coût métier est significativement plus élevé, ce qui rend ce modèle moins compétitif.\n",
    "\n",
    "### Modèle : Logistic Regression\n",
    "- **Seuil Optimal** : 0.08  \n",
    "- **Coût Métier Optimal** : 52,310.00  \n",
    "\n",
    "**Analyse** :  \n",
    "La régression logistique bénéficie également du rééquilibrage. L'AUC atteint 0.976, l'accuracy est de 81.9 %, et la précision est bonne (0.745). Cependant, le rappel (0.971) et le coût métier (52,310.00) indiquent une performance légèrement inférieure à LightGBM.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion \n",
    "\n",
    "### **Meilleur modèle global : LightGBM **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18a0f0-f7e5-4411-ade3-6982242c923c",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 4 : Analyse des features du meilleur modèle </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5927c2c-e719-4cbd-9354-2f6709894dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"{title} - terminé en {time.time() - t0:.2f}s\")\n",
    "\n",
    "def analyse_shap(model_file, train_df, feats, output_dir=\"shap_results\"):\n",
    "    \"\"\"\n",
    "    Analyse SHAP avec un modèle LightGBM déjà entraîné.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Chargement du modèle LightGBM pour SHAP...\")\n",
    "    clf = lgb.Booster(model_file=model_file)\n",
    "\n",
    "    print(\"Préparation des données pour SHAP...\")\n",
    "    data_sample = train_df.sample(10000, random_state=42)[feats]\n",
    "    print(f\"Échantillon : {data_sample.shape}\")\n",
    "\n",
    "    print(\"Calcul des valeurs SHAP...\")\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(data_sample)\n",
    "\n",
    "    # Vérification pour modèles binaires\n",
    "    if isinstance(shap_values, list) and len(shap_values) > 1:\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    print(\"Création des graphiques SHAP...\")\n",
    "    # Résumé SHAP (violin plot)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, data_sample, plot_type=\"violin\", show=False)\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_violin.png\"))\n",
    "    plt.show()  # Affiche le graphique dans le notebook\n",
    "    plt.close()\n",
    "\n",
    "    # Résumé SHAP (bar plot)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, data_sample, plot_type=\"bar\", show=False)\n",
    "    plt.savefig(os.path.join(output_dir, \"shap_summary_bar.png\"))\n",
    "    plt.show()  # Affiche le graphique dans le notebook\n",
    "    plt.close()\n",
    "\n",
    "    # Force plot pour un client spécifique\n",
    "    print(\"Création du graphique SHAP pour un individu (force plot)...\")\n",
    "    sample_idx = 35  # Vous pouvez changer cet indice pour un autre client\n",
    "    \n",
    "    # Vérifier si expected_value est une liste ou une valeur scalaire\n",
    "    expected_value = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "    \n",
    "    # Générer le force plot\n",
    "    shap.force_plot(\n",
    "        expected_value,  # Utiliser la valeur attendue appropriée\n",
    "        shap_values[sample_idx, :],  # Valeurs SHAP pour l'individu\n",
    "        data_sample.iloc[sample_idx, :],  # Données de l'individu\n",
    "        matplotlib=True\n",
    "    )\n",
    "    plt.savefig(os.path.join(output_dir, f\"shap_force_plot_client_{sample_idx}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def main(debug=False):\n",
    "    print(\"Chargement des données...\")\n",
    "    df = application_train_test(10000 if debug else None)  # Remplacez par votre fonction de chargement\n",
    "    df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True).str.replace(' ', '_')\n",
    "\n",
    "    # Séparer les données d'entraînement\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index']]\n",
    "\n",
    "    # Charger un modèle déjà entraîné\n",
    "    model_file = \"lightgbm_model_final.txt\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"Erreur : le fichier du modèle '{model_file}' est introuvable. Veuillez entraîner et sauvegarder le modèle au préalable.\")\n",
    "        return\n",
    "\n",
    "    # Analyse SHAP\n",
    "    with timer(\"Analyse SHAP\"):\n",
    "        analyse_shap(model_file, train_df, feats)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Exécution complète\"):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2cff1-0ddb-4268-a68b-abec2d2872f3",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 5 : Data drift </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17588b4c-715c-4ea3-90dc-042d4dca2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Charger les datasets\n",
    "df_preprocessed = application_train_test(nan_as_category=True)\n",
    "\n",
    "# Séparer les données d'entraînement (TARGET non nul)\n",
    "train_df = df_preprocessed[df_preprocessed['TARGET'].notnull()]\n",
    "\n",
    "# Charger les données de test\n",
    "test_df = df_preprocessed[df_preprocessed['TARGET'].isnull()]\n",
    "\n",
    "# Charger le modèle LightGBM\n",
    "model_file = \"lightgbm_model_final.txt\"\n",
    "if not os.path.exists(model_file):\n",
    "    raise FileNotFoundError(f\"Le fichier de modèle '{model_file}' est introuvable.\")\n",
    "\n",
    "clf = lgb.Booster(model_file=model_file)\n",
    "\n",
    "# Extraire les features importantes à partir du modèle\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': clf.feature_name(),\n",
    "    'importance': clf.feature_importance(importance_type='gain')\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Sélectionner les 20 features les plus importantes\n",
    "top_features = feature_importances['feature'].head(20).tolist()\n",
    "\n",
    "# Filtrer les datasets sur les colonnes importantes\n",
    "train_df_filtered = train_df[available_features]\n",
    "test_df_filtered = test_df[available_features]\n",
    "\n",
    "# Création d'un rapport Evidently pour le Data Drift\n",
    "data_drift_report = Report(metrics=[DataDriftPreset()])\n",
    "\n",
    "# Calculer le Data Drift entre les données d'entraînement et de test\n",
    "data_drift_report.run(reference_data=train_df_filtered, current_data=test_df_filtered)\n",
    "\n",
    "# Sauvegarder le rapport HTML\n",
    "output_file = \"data_drift_report_top_features.html\"\n",
    "data_drift_report.save_html(output_file)\n",
    "\n",
    "print(f\"Le rapport de Data Drift a été généré : {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00e138-b529-4f82-aa32-04c9c7ec22d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
