{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52418d18-d703-4c91-a27b-5489e6b63069",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">\n",
    "    <span style=\"font-size: xx-large; font-weight: bold; color: red;\">\n",
    "        PROJET 7 :\n",
    "        Implementez un modèle de scoring\n",
    "    </span>\n",
    "</p>  \n",
    "\n",
    "__Sommaire__\n",
    "\n",
    "**Partie 1 : Importation des bibliothèques**\n",
    "\n",
    "**Partie 2 : Analyse exploratoire des données**\n",
    "- <a href=\"#C0\">Fonctions utiles</a>\n",
    "- <a href=\"#C1\">Chargement des fichiers</a>\n",
    "- <a>Prétraitements des fichiers</a>\n",
    "    - <a href=\"#C2\"> Test et validation</a>\n",
    "    - <a href=\"#C3\"> bureau et bureau_balance</a>\n",
    "    - <a href=\"#C4\"> previous_application</a>\n",
    "    - <a href=\"#C5\"> posh_cash_balance</a>\n",
    "    - <a href=\"#C6\"> installments_payement</a>\n",
    "    - <a href=\"#C7\"> credit_card_balance</a>\n",
    "- <a>Feature engineering (conservation des features qui ont moins de 80% de données manquantes)</a>\n",
    "    - <a href=\"#C8\"> df </a>\n",
    "    - <a href=\"#C9\"> bureau_agg</a>\n",
    "    - <a href=\"#C10\">previous_applications_agg </a>\n",
    "    - <a href=\"#C11\">installments_payments_agg </a>\n",
    "    - <a href=\"#C12\">credit_card_balance_agg</a>\n",
    "    - <a href=\"#C13\">home_credit </a>\n",
    "- <a>Exploration variable cible</a>\n",
    "    - <a href=\"#C14\">feature target </a>\n",
    "\n",
    "**Partie 3 : Modélisation**   \n",
    "- <a href=\"#C15\">Prétraitement des données </a>\n",
    "- <a href=\"#C16\">Traitement données train avec/sans réequilibrage</a>\n",
    "- <a href=\"#C17\">Modelisation</a>\n",
    "- <a href=\"#C18\">Configuration environnement MLflow et entrainement modèle</a>\n",
    "- <a href=\"#C19\">Optimisation des hyperparametres</a>\n",
    "- <a href=\"#C20\">Evaluation des modèles</a>\n",
    "- <a href=\"#C21\">Resultat des modèles</a>\n",
    "\n",
    "**Partie 4 : Analyse des features du meilleur modèle**   \n",
    "- <a>Analyse local et global à partir de shap</a>\n",
    "\n",
    "**Partie 5 : Data drift**   \n",
    "- <a>generation rapport data drift à partir d’evidently </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600f2e5-0353-44dd-b1d7-9af54017df11",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 1 : Importation des bibliothèques</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "1e1df803-f0fc-4ec3-8c1c-cb5f72916f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn Modules\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, KFold, cross_val_score, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Evidently (for Data Drift Analysis)\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "# Joblib (for Model Saving/Loading)\n",
    "import joblib\n",
    "\n",
    "# Warnings Configuration\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# mlflow ui --port 6000\n",
    "#kill -9 PID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def46bf6-2f90-4b61-87db-1793249b9747",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 2 : Analyse exploratoire des données</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f50e32-c508-4c88-a80a-5abe1de0987e",
   "metadata": {},
   "source": [
    "# <a name=\"C0\"><span style=\"text-decoration: underline;\">Fonctions utiles</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "ec05a219-ab49-485a-a156-6a19ea16ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate missing values by column# Funct \n",
    "def missing_values_table(df):\n",
    "        # Total missing values\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # Percentage of missing values\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # Make a table with the results\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # Rename the columns\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # Sort the table by percentage of missing descending\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # Print some summary information\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "431abd76-a5ec-4ec7-af79-abcf460259c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c5f3-175f-4974-a844-c2e422956fdd",
   "metadata": {},
   "source": [
    "# <a name=\"C1\"><span style=\"text-decoration: underline;\">Chargement des fichiers</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "7e07de4f-d46b-49fa-bba8-4861bdf31ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les fichiers en utilisant dask\n",
    "sample_submission = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/sample_submission.csv\")\n",
    "home_credit = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/HomeCredit_columns_description.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dc1cb-45fc-46e0-b049-8b1ad4fb967b",
   "metadata": {},
   "source": [
    "# <a name=\"C2\"><span style=\"text-decoration: underline;\">Prétraitement des fichiers de validation et d'entrainement</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "73c4b176-3da0-43d8-abaa-65cd8d5962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des fichiers application_train.csv et application_test.csv sans limitation de lignes\n",
    "def application_train_test(nan_as_category=False):\n",
    "    # Lire les données complètes et fusionner\n",
    "    df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_train.csv\")\n",
    "    test_df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_test.csv\")\n",
    "    print(\"Échantillons d'entraînement : {}, échantillons de test : {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index(drop=True)\n",
    "    \n",
    "    # Optionnel : Supprimer les applications avec CODE_GENDER 'XNA' (ensemble d'entraînement)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Caractéristiques catégorielles avec encodage binaire (0 ou 1 ; deux catégories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "        \n",
    "    # Caractéristiques catégorielles avec encodage One-Hot\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # Valeurs NaN pour DAYS_EMPLOYED : 365.243 -> NaN\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Quelques nouvelles caractéristiques simples (pourcentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    \n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463a0dd-83b7-406d-9457-ad83a463f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appel de la fonction pour voir le résultat\n",
    "df = application_train_test(nan_as_category=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80afb9cc-c5b5-4579-af44-79815f83adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training/test data shape: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe9ebc-e0e7-47a7-a108-809507048b04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(df)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffb7e9-80d6-4458-b932-93653f8a0af9",
   "metadata": {},
   "source": [
    "Dans des travaux ultérieurs, nous utiliserons des modèles tels que XGBoost qui peuvent gérer les valeurs manquantes sans avoir besoin d’imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20add28a-8b16-4256-8048-1925a19ae264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre des differents types\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445230e0-95f1-4aef-add8-79f7511943e4",
   "metadata": {},
   "source": [
    "# <a name=\"C3\"><span style=\"text-decoration: underline;\">Prétraitement des données bureau et bureau_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fbb68-b107-4d60-b4d5-acecf4ef902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des fichiers bureau.csv et bureau_balance.csv\n",
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/bureau.csv\")\n",
    "    bb = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/bureau_balance.csv\")\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance : Effectuer des agrégations et fusionner avec bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Caractéristiques numériques de bureau et bureau_balance\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Caractéristiques catégorielles de bureau et bureau_balance\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau : Crédits actifs - en utilisant uniquement les agrégations numériques\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau : Crédits clôturés - en utilisant uniquement les agrégations numériques\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0ef24-9601-4370-b99c-2c0935eac1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et afficher le DataFrame résultant\n",
    "bureau_agg = bureau_and_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf4acf-2e8b-417d-bb83-afed9fc05864",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd8e75-3d20-4add-a124-2223300a0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bureau/bureau_balance : ', bureau_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e91d4-c304-416f-8bbe-e535a3977583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(bureau_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c34632-46bc-4272-8e48-892347ef21d7",
   "metadata": {},
   "source": [
    "# <a name=\"C4\"><span style=\"text-decoration: underline;\">Prétraitement des données de previous_application</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65519a9d-f9b8-42f6-9b66-4dab9ab7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'encodage One-Hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    df = pd.get_dummies(df, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Prétraitement de previous_applications.csv\n",
    "def previous_applications(nan_as_category=True):\n",
    "    prev = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/previous_application.csv\")\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "    \n",
    "    # Valeurs de 365.243 jours -> NaN\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Ajouter une caractéristique : pourcentage de la valeur demandée / valeur reçue\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    \n",
    "    # Caractéristiques numériques des demandes précédentes\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Caractéristiques catégorielles des demandes précédentes\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    \n",
    "    # Demandes précédentes : Demandes approuvées - uniquement les caractéristiques numériques\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    # Demandes précédentes : Demandes refusées - uniquement les caractéristiques numériques\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    \n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1159ab4-e9e7-447f-ab11-4fa2c2e74ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter la fonction et afficher le DataFrame\n",
    "previous_applications_agg = previous_applications()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b89cdc-cc94-4cbb-a21d-6e1c8ee56190",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_applications_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e5699-f11b-4f0c-9b59-1223fa4d7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('previous : ', previous_applications_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f381e-2f93-4b07-9dab-ff06154f5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(previous_applications_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5847a97-b515-4ae3-b23d-0f2062c49670",
   "metadata": {},
   "source": [
    "# <a name=\"C5\"><span style=\"text-decoration: underline;\">Prétraitement des données de POS_CASH_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8b437-9a3e-444f-a1de-8588fff9799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement de POS_CASH_balance.csv\n",
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/POS_CASH_balance.csv\")\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "    \n",
    "    # Caractéristiques\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de comptes POS cash\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250bc92-cf13-4620-b456-b7da581d68f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "pos_cash_agg = pos_cash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856d667-45e1-469b-ae5d-ab7b893306df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50972410-4c32-4f3c-bfb0-af437b04440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pos_cash : ', pos_cash_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c5008-f395-4991-ba10-24e2d4fc8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(pos_cash_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f063cc-0b88-41b7-809f-a3f5a808a03d",
   "metadata": {},
   "source": [
    "# <a name=\"C6\"><span style=\"text-decoration: underline;\">Prétraitement des données installments_payments</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e255d-eecd-4f58-b0bc-85eeab0e922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'encodage One-Hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    df = pd.get_dummies(df, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Prétraitement de installments_payments.csv\n",
    "def installments_payments(nan_as_category=True):\n",
    "    # Lire les données de installments_payments\n",
    "    ins = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/installments_payments.csv\")\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "    \n",
    "    # Pourcentage et différence payés dans chaque versement (montant payé et valeur du versement)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    \n",
    "    # Jours de retard et jours avant échéance (pas de valeurs négatives)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    \n",
    "    # Caractéristiques : effectuer des agrégations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de comptes de versements\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # Libérer la mémoire\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    \n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5fc28-1e29-4005-acdf-e4868c24be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "installments_payments_agg = installments_payments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8b0f4-5dc9-4095-af7d-995c3b02a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "installments_payments_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb20c2d-65a2-4900-af33-609ed29ed3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('installments payments : ', installments_payments_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76797df-81dc-44bb-8448-3bc9a1898a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(installments_payments_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41afc620-c8bd-47f3-91f9-9254d7223c0d",
   "metadata": {},
   "source": [
    "# <a name=\"C7\"><span style=\"text-decoration: underline;\">Prétraitement des données credit_card_balance</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd6dc0-0ad4-4b1c-8fa1-73d00ee9688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement de credit_card_balance.csv\n",
    "def credit_card_balance(nan_as_category=True):\n",
    "    cc = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/credit_card_balance.csv\")\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "    \n",
    "    # Agrégations générales\n",
    "    cc.drop(['SK_ID_PREV'], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    \n",
    "    # Compter le nombre de lignes de carte de crédit\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c63f54-da83-4e49-ac08-2e28c0a3e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger et visualiser le DataFrame\n",
    "credit_card_balance_agg = credit_card_balance()\n",
    "credit_card_balance_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cca21-88d1-4205-8e63-2326dbb6e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('credit_card_balance_agg : ', credit_card_balance_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22577e1b-f47a-47d7-99f4-ed8a49326ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(credit_card_balance_agg)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e16ada-d572-4977-bf0b-0e8d6a5b2e00",
   "metadata": {},
   "source": [
    "**sample_submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85702fa-6bc6-4c00-9bc9-6d96bf3fee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sample_submission : ', sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29894bd3-ed75-4542-92d8-f5e4860fe218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(sample_submission)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c157216-fca2-498d-9d86-bb14ad109880",
   "metadata": {},
   "source": [
    "**HomeCredit_columns_description**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc1e38-4554-42cf-bb4e-9d0c3e5645fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('home_credit : ', home_credit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342ff7b-c309-479a-a2ef-9a7eb4dd2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values statistics\n",
    "missing_values = missing_values_table(home_credit)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4550f13-6fce-4e33-9490-175e062dc9e6",
   "metadata": {},
   "source": [
    "# <a name=\"C8\"><span style=\"text-decoration: underline;\">Traitement données manquantes df</span></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58892a8-7a86-4ff5-af1e-13134fec5f06",
   "metadata": {},
   "source": [
    "les data frames qui ont des données manquantes : \n",
    "- df\n",
    "- bureau_agg\n",
    "- previous_applications_agg\n",
    "- installments_payments_agg\n",
    "- credit_card_balance_agg\n",
    "- home_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbe9b8-174d-41a6-bc48-64cb5bacd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(df)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbde27-f775-47ed-879c-2e0285fff27a",
   "metadata": {},
   "source": [
    "# <a name=\"C9\"><span style=\"text-decoration: underline;\">Traitement données manquantes bureau_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992166c-a3ca-4f71-a8be-832c646a062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(bureau_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59526c3b-30a7-4f6e-b314-374e5d77fd3a",
   "metadata": {},
   "source": [
    "# <a name=\"C10\"><span style=\"text-decoration: underline;\">Traitement données manquantes previous_applications_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85d40f-d957-446c-9f59-8153abb1e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(previous_applications_agg)\n",
    "\n",
    "# Filtrer pour obtenir les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "features_to_drop = missing_values[missing_values['% of Total Values'] > 80].index\n",
    "\n",
    "# Supprimer les colonnes avec plus de 80 % de valeurs manquantes du DataFrame principal\n",
    "previous_applications_agg_cleaned = previous_applications_agg.drop(columns=features_to_drop)\n",
    "\n",
    "# Afficher la forme du DataFrame après la suppression\n",
    "print(\"DataFrame initial :\", previous_applications_agg.shape)\n",
    "print(\"DataFrame après suppression des colonnes avec plus de 80 % de valeurs manquantes :\", previous_applications_agg_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846978cf-2f9b-43f1-8845-fbf0a58da77e",
   "metadata": {},
   "source": [
    "# <a name=\"C11\"><span style=\"text-decoration: underline;\">Traitement données manquantes installments_payments_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d56b6-be5b-4813-a26e-b4a1c4df8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(installments_payments_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9699d7-725c-4a5a-976c-cefef1d71d5d",
   "metadata": {},
   "source": [
    "# <a name=\"C12\"><span style=\"text-decoration: underline;\">Traitement données manquantes credit_card_balance_agg</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedea3b-f17b-4bf0-bfc2-b32699395e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(credit_card_balance_agg)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20340ba4-8ac1-4984-a832-24c0396e612e",
   "metadata": {},
   "source": [
    "# <a name=\"C13\"><span style=\"text-decoration: underline;\">Traitement données manquantes home_credit</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470b079-8ad9-47e3-967e-f8f2600edbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(home_credit)\n",
    "\n",
    "# Filtrer pour obtenir uniquement les caractéristiques avec plus de 80 % de valeurs manquantes\n",
    "missing_values_80 = missing_values[missing_values['% of Total Values'] > 80]\n",
    "\n",
    "# Afficher les premières lignes des colonnes avec plus de 80 % de valeurs manquantes\n",
    "print(missing_values_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27d387-59e6-450a-87e8-23cc9e115a3c",
   "metadata": {},
   "source": [
    "# <a name=\"C14\"><span style=\"text-decoration: underline;\">Exploration variable cible (target)</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c076c12-066f-4712-a1a0-b0718683634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a4bf6-4b20-4bc5-b59c-2a4dba6dd691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anomalous flag column\n",
    "df['DAYS_EMPLOYED_ANOM'] = df[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "# Replace the anomalous values with nan\n",
    "df['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "\n",
    "df['DAYS_EMPLOYED'].plot.hist(title = 'Histogramme de l’emploi par jour');\n",
    "plt.xlabel('Jours d’emploi');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48177cc-585e-4a65-8ef3-f515fdf3b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver les corrélations avec la cible et trier\n",
    "correlations = df.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Afficher les corrélations\n",
    "print('Corrélations les plus positives :\\n', correlations.tail(15))\n",
    "print('\\nCorrélations les plus négatives :\\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf9bbc-dccb-448a-97cd-ab3d3a20f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations d'âge dans un DataFrame séparé\n",
    "age_data = df[['TARGET', 'DAYS_BIRTH']]\n",
    "\n",
    "# Prendre la valeur absolue de DAYS_BIRTH pour enlever les valeurs négatives, puis convertir en années\n",
    "age_data['DAYS_BIRTH'] = age_data['DAYS_BIRTH'].abs()  # Cette ligne prend la valeur absolue\n",
    "age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n",
    "\n",
    "# Créer des intervalles d'âge personnalisés de 5 en 5 ans\n",
    "bins = [20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70]\n",
    "age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins=bins)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(age_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc64b9-cb0f-4b63-a05c-66a871651b61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by the bin and calculate averages\n",
    "age_groups  = age_data.groupby('YEARS_BINNED').mean()\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb58ba-76fb-44f5-850e-69f7794cdef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Graphique des tranches d'âge et de la moyenne de la cible sous forme de diagramme à barres\n",
    "plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n",
    "\n",
    "# Étiquettes du graphique\n",
    "plt.xticks(rotation = 75)\n",
    "plt.xlabel('Groupe d\\'âge (années)')\n",
    "plt.ylabel('Défaut de remboursement (%)')\n",
    "plt.title('Défaut de remboursement par groupe d\\'âge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2e564-1ef0-4575-a812-ce6745028159",
   "metadata": {},
   "source": [
    "Il y a une tendance claire : les jeunes demandeurs sont plus susceptibles de ne pas rembourser le prêt! Le taux de non-remboursement est supérieur à 10 % pour les trois groupes d’âge les plus jeunes et inférieur à 5 % pour le groupe d’âge le plus âgé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf27a4-b790-4374-b0a6-1d2cb5ea648c",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 3 : Préparation des données pour la modélisation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc75f4-849a-4405-a9ed-2c09f99db955",
   "metadata": {},
   "source": [
    "# <a name=\"C15\"><span style=\"text-decoration: underline;\">Prétraitement des données</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e164e4-211d-4a9f-91a2-e7db57a0e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df = pd.read_csv(\"/Users/Nelly/Desktop/projet 7/data/application_train.csv\")\n",
    "\n",
    "# Fonction d'encodage one-hot\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Appliquer l'encodage one-hot\n",
    "df, new_columns = one_hot_encoder(df, nan_as_category=True)\n",
    "\n",
    "# Vérifier les colonnes disponibles\n",
    "print(\"Colonnes disponibles :\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4d551-0be5-41c3-be91-8475c67ad33d",
   "metadata": {},
   "source": [
    "# <a name=\"C16\"><span style=\"text-decoration: underline;\">Traitement données train avec/sans réequilibrage</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6e2c60-8a74-4b10-8f6a-71662f8f6bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Préparer les données pour smote\n",
    "X = df.drop(columns=['TARGET'])  # Features\n",
    "y = df['TARGET']  # Cible\n",
    "\n",
    "# Vérifier les valeurs manquantes\n",
    "print(\"Nombre de valeurs manquantes avant imputation :\", X.isnull().sum().sum())\n",
    "\n",
    "# Imputer les valeurs manquantes\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Vérifier les proportions dans le jeu déséquilibré\n",
    "print(\"Proportions avant rééquilibrage :\", Counter(y))\n",
    "\n",
    "# Diviser les données en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Créer une copie des données sans rééquilibrage\n",
    "df_no_balancing = pd.DataFrame(X_train, columns=X.columns)\n",
    "df_no_balancing['TARGET'] = y_train\n",
    "\n",
    "# Appliquer SMOTE pour le rééquilibrage des données d'entraînement\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Vérifier les proportions après SMOTE\n",
    "print(\"Proportions après SMOTE :\", Counter(y_train_resampled))\n",
    "\n",
    "# Créer le DataFrame avec rééquilibrage\n",
    "df_balanced = pd.DataFrame(X_train_resampled, columns=X.columns)\n",
    "df_balanced['TARGET'] = y_train_resampled\n",
    "\n",
    "# Sauvegarder les DataFrames\n",
    "# df_no_balancing.to_csv('/Users/Nelly/Desktop/projet 7/data/saved_train_no_balancing_smote.csv', index=False)\n",
    "# df_balanced.to_csv('/Users/Nelly/Desktop/projet 7/data/saved_train_balanced_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff365b8-4fa4-4615-b18d-eaca25aa5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['TARGET'])\n",
    "X_train_resampled = X_train_resampled.drop(columns=['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bacc23-7bce-4de4-b7bf-a5932e866e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f150a-8635-4871-80bc-a69b4fb34fe1",
   "metadata": {},
   "source": [
    "# <a name=\"C17\"><span style=\"text-decoration: underline;\">Modélisation</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5309e-4ce4-430e-a277-5c8d213862cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs manquantes pour chaque dataset\n",
    "missing_no = X_train.isnull().sum()  # Somme des valeurs manquantes par colonne\n",
    "missing_bal = X_train_resampled.isnull().sum()    # Somme des valeurs manquantes par colonne\n",
    "\n",
    "# Proportion des données manquantes en pourcentage\n",
    "missing_no_percent = (missing_no / len(X_train)) * 100\n",
    "missing_bal_percent = (missing_bal / len(X_train_resampled)) * 100\n",
    "\n",
    "print(\"Pourcentage des données manquantes (sans rééquilibrage) :\")\n",
    "print(missing_no)\n",
    "\n",
    "print(\"\\nPourcentage des données manquantes (avec rééquilibrage) :\")\n",
    "print(missing_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55267f4-6b6f-4be7-9cd3-b5e109abfc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer le coût métier\n",
    "def calculate_business_cost(y_true, y_pred, fn_cost=10, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Calcule le coût métier basé sur les faux négatifs (FN) et faux positifs (FP).\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    business_cost = fn * fn_cost + fp * fp_cost\n",
    "    return business_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b6e55-b3ca-4ac1-ba78-4d877e47b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Fonction pour trouver le seuil optimal\n",
    "def find_optimal_threshold(y_true, y_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    optimal_idx = (tpr - fpr).argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "# Évaluation sur les données SANS rééquilibrage\n",
    "print(\"=== DummyClassifier (Données SANS rééquilibrage) ===\")\n",
    "dummy_clf_no = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "dummy_clf_no.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_dummy_no = dummy_clf_no.predict(X_test)\n",
    "y_proba_dummy_no = dummy_clf_no.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Trouver le seuil optimal\n",
    "optimal_threshold_no = find_optimal_threshold(y_test, y_proba_dummy_no)\n",
    "y_pred_dummy_optimal_no = (y_proba_dummy_no >= optimal_threshold_no).astype(int)\n",
    "\n",
    "# Évaluation des performances\n",
    "dummy_accuracy_no = accuracy_score(y_test, y_pred_dummy_no)\n",
    "dummy_auc_no = roc_auc_score(y_test, y_proba_dummy_no)\n",
    "dummy_precision_no = precision_score(y_test, y_pred_dummy_optimal_no, zero_division=0)\n",
    "dummy_recall_no = recall_score(y_test, y_pred_dummy_optimal_no, zero_division=0)\n",
    "dummy_f1_no = f1_score(y_test, y_pred_dummy_optimal_no, zero_division=0)\n",
    "dummy_cost_no = calculate_business_cost(y_test, y_pred_dummy_no, fn_cost=10, fp_cost=1)\n",
    "\n",
    "print(f\"Accuracy (no balancing): {dummy_accuracy_no:.4f}\")\n",
    "print(f\"AUC (no balancing): {dummy_auc_no:.4f}\")\n",
    "print(f\"Precision (no balancing): {dummy_precision_no:.4f}\")\n",
    "print(f\"Recall (no balancing): {dummy_recall_no:.4f}\")\n",
    "print(f\"F1-Score (no balancing): {dummy_f1_no:.4f}\")\n",
    "print(f\"Optimal Threshold (no balancing): {optimal_threshold_no:.4f}\")\n",
    "print(f\"Business Cost (no balancing): {dummy_cost_no}\")\n",
    "\n",
    "# Évaluation sur les données AVEC rééquilibrage\n",
    "print(\"\\n=== DummyClassifier (Données AVEC rééquilibrage) ===\")\n",
    "dummy_clf_bal = DummyClassifier(strategy=\"most_frequent\", random_state=42)\n",
    "dummy_clf_bal.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_dummy_bal = dummy_clf_bal.predict(X_train_resampled)\n",
    "y_proba_dummy_bal = dummy_clf_bal.predict_proba(X_train_resampled)[:, 1]\n",
    "\n",
    "# Trouver le seuil optimal\n",
    "optimal_threshold_bal = find_optimal_threshold(y_train_resampled, y_proba_dummy_bal)\n",
    "y_pred_dummy_optimal_bal = (y_proba_dummy_bal >= optimal_threshold_bal).astype(int)\n",
    "\n",
    "# Évaluation des performances\n",
    "dummy_accuracy_bal = accuracy_score(y_train_resampled, y_pred_dummy_bal)\n",
    "dummy_auc_bal = roc_auc_score(y_train_resampled, y_proba_dummy_bal)\n",
    "dummy_precision_bal = precision_score(y_train_resampled, y_pred_dummy_optimal_bal, zero_division=0)\n",
    "dummy_recall_bal = recall_score(y_train_resampled, y_pred_dummy_optimal_bal, zero_division=0)\n",
    "dummy_f1_bal = f1_score(y_train_resampled, y_pred_dummy_optimal_bal, zero_division=0)\n",
    "dummy_cost_bal = calculate_business_cost(y_train_resampled, y_pred_dummy_bal, fn_cost=10, fp_cost=1)\n",
    "\n",
    "print(f\"Accuracy (balancing): {dummy_accuracy_bal:.4f}\")\n",
    "print(f\"AUC (balancing): {dummy_auc_bal:.4f}\")\n",
    "print(f\"Precision (balancing): {dummy_precision_bal:.4f}\")\n",
    "print(f\"Recall (balancing): {dummy_recall_bal:.4f}\")\n",
    "print(f\"F1-Score (balancing): {dummy_f1_bal:.4f}\")\n",
    "print(f\"Optimal Threshold (balancing): {optimal_threshold_bal:.4f}\")\n",
    "print(f\"Business Cost (balancing): {dummy_cost_bal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c618d3-3ee0-4703-914d-b3abef76d418",
   "metadata": {},
   "source": [
    "# <a name=\"C18\"><span style=\"text-decoration: underline;\">Configuration environnement MLflow et entrainement modèle</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e08784-8401-4a02-a023-b3a42c2f7c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Configurez MLflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Credit Scoring Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a680f-508d-4c9f-a9d4-227ff671176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, model_type, X_train, X_test, y_train, y_test):\n",
    "    print(\"Début de la fonction objective\")\n",
    "\n",
    "    # Démarrer un run MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_type} optimization\"):\n",
    "        print(\"Run MLflow démarré\")\n",
    "        active_run = mlflow.active_run()\n",
    "        if active_run:\n",
    "            print(f\"Run ID actif : {active_run.info.run_id}\")\n",
    "        else:\n",
    "            print(\"Erreur : Aucun run actif détecté !\")\n",
    "\n",
    "        # Nettoyage des colonnes\n",
    "        print(\"Nettoyage des colonnes...\")\n",
    "        X_train = X_train.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "        X_test = X_test.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "\n",
    "        # Initialisation du modèle\n",
    "        print(f\"Initialisation du modèle {model_type}...\")\n",
    "        if model_type == \"LightGBM\":\n",
    "            model = lgb.LGBMClassifier(\n",
    "                n_estimators=int(params.get('n_estimators', 100)),\n",
    "                num_leaves=int(params.get('num_leaves', 31)),\n",
    "                learning_rate=params.get('learning_rate', 0.1),\n",
    "                max_depth=int(params.get('max_depth', -1)),\n",
    "                subsample=params.get('subsample', 1.0),\n",
    "                colsample_bytree=params.get('colsample_bytree', 1.0),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif model_type == \"RandomForest\":\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=int(params.get('n_estimators', 100)),\n",
    "                max_depth=int(params.get('max_depth', None)),\n",
    "                min_samples_split=params.get('min_samples_split', 2),\n",
    "                min_samples_leaf=params.get('min_samples_leaf', 1),\n",
    "                random_state=42\n",
    "            )\n",
    "        elif model_type == \"LogisticRegression\":\n",
    "            model = LogisticRegression(\n",
    "                C=params.get('C', 1.0),\n",
    "                solver=params.get('solver', 'lbfgs'),\n",
    "                max_iter=1000,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Type de modèle non supporté : {}\".format(model_type))\n",
    "\n",
    "        print(\"Entraînement du modèle...\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        print(\"Calcul des métriques...\")\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Calcul des métriques pertinentes\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # Calcul du coût métier\n",
    "        business_cost = calculate_business_cost(y_test, y_pred)\n",
    "\n",
    "        # Log des métriques dans MLflow\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.log_metric(\"Accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"Precision\", precision)\n",
    "        mlflow.log_metric(\"Recall\", recall)\n",
    "        mlflow.log_metric(\"F1-Score\", f1)\n",
    "        mlflow.log_metric(\"Business Cost\", business_cost)\n",
    "\n",
    "        print(f\"Métriques loguées : AUC={auc}, Accuracy={accuracy}, Precision={precision}, Recall={recall}, F1-Score={f1}, Business Cost={business_cost}\")\n",
    "\n",
    "        # Enregistrement du modèle dans MLflow\n",
    "        mlflow.sklearn.log_model(model, f\"{model_type}_Model\")\n",
    "\n",
    "        print(f\"Run ID final pour le Model Registry : {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "        return {\n",
    "            'loss': -auc,  # AUC négatif pour minimisation\n",
    "            'status': STATUS_OK,\n",
    "            'auc': auc,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'business_cost': business_cost\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d4fe2-0463-4e59-a9e2-99fbdf01cfca",
   "metadata": {},
   "source": [
    "# <a name=\"C19\"><span style=\"text-decoration: underline;\">Optimisation des hyperparametres des modèles</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02267b97-8de8-4742-a260-d5639003f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espace des hyperparamètres\n",
    "param_space_lgb = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 50, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'max_depth': hp.quniform('max_depth', -1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bba88-afd9-4248-9c2a-77dd847f02dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Définir les jeux de données\n",
    "datasets = {\n",
    "    \"Sans rééquilibrage\": (X_train, X_test, y_train, y_test),\n",
    "    \"Avec rééquilibrage\": (X_train_resampled,X_test,y_train_resampled,y_test)\n",
    "}\n",
    "\n",
    "# Tester chaque jeu de données\n",
    "for name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"Optimisation pour le dataset : {name}\")\n",
    "    \n",
    "    trials_lgb = Trials()\n",
    "    best_params_lgb = fmin(\n",
    "        fn=lambda params: objective(params, \"LightGBM\", X_train, X_test, y_train, y_test),\n",
    "        space=param_space_lgb,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=5,\n",
    "        trials=trials_lgb\n",
    "    )\n",
    "    \n",
    "    print(f\"Meilleurs hyperparamètres pour {name} :\", best_params_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7b4401-3e73-42cf-9770-8e37dd518c6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres pour Random Forest\n",
    "print(\"Optimisation des hyperparamètres pour Random Forest...\")\n",
    "\n",
    "param_space_rf = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 200, 10),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 30, 1),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5)\n",
    "}\n",
    "\n",
    "trials_rf = Trials()\n",
    "best_params_rf = fmin(\n",
    "    fn=lambda params: objective(params, \"RandomForest\", X_train, X_test, y_train, y_test),\n",
    "    space=param_space_rf,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_rf\n",
    ")\n",
    "\n",
    "print(\"Meilleurs hyperparamètres pour Random Forest :\", best_params_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae1b38-953d-4f45-ada4-a0980ef4519b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimisation des hyperparamètres pour Logistic Regression\n",
    "print(\"Optimisation des hyperparamètres pour Logistic Regression...\")\n",
    "\n",
    "param_space_lr = {\n",
    "    'C': hp.loguniform('C', -4, 2),  # Exponentielle pour couvrir une large plage\n",
    "    'solver': hp.choice('solver', ['lbfgs', 'liblinear', 'saga'])\n",
    "}\n",
    "\n",
    "trials_lr = Trials()\n",
    "best_params_lr = fmin(\n",
    "    fn=lambda params: objective(params, \"LogisticRegression\", X_train, X_test, y_train, y_test),\n",
    "    space=param_space_lr,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_lr\n",
    ")\n",
    "\n",
    "print(\"Meilleurs hyperparamètres pour Logistic Regression :\", best_params_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354f44c-15ec-449e-84bf-184c6680f230",
   "metadata": {},
   "source": [
    "# <a name=\"C20\"><span style=\"text-decoration: underline;\">Evaluation des modèles</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4886762-4a82-4c6f-ba6e-fa98015648cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonisation des noms de colonnes (nettoyage des caractères spéciaux)\n",
    "X_train.columns = X_train.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "X_test.columns = X_test.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "X_train_resampled.columns = X_train_resampled.columns.str.replace(r\"[^\\w]\", \"_\", regex=True)\n",
    "\n",
    "# Fonction pour évaluer un modèle optimisé et le sauvegarder\n",
    "def evaluate_optimized_model(model, X_train, X_test, y_train, y_test, model_name=None, save_path=None, fn_cost=10, fp_cost=1):\n",
    "    \"\"\"\n",
    "    Entraîne et évalue un modèle, calcule les métriques, et sauvegarde des modèles\n",
    "    \"\"\"\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    best_threshold = 0.5\n",
    "    best_cost = float('inf')\n",
    "    results_by_threshold = []\n",
    "\n",
    "    # Calcul des métriques à différents seuils\n",
    "    thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        cost = calculate_business_cost(y_test, y_pred, fn_cost, fp_cost)\n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "            best_threshold = threshold\n",
    "\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        results_by_threshold.append({\n",
    "            \"Threshold\": threshold,\n",
    "            \"AUC\": auc,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1,\n",
    "            \"Business Cost\": cost\n",
    "        })\n",
    "\n",
    "    # Sauvegarde du modèle si les paramètres sont fournis\n",
    "    if save_path and model_name:\n",
    "        os.makedirs(save_path, exist_ok=True)  # Créer le dossier si inexistant\n",
    "        save_file = f\"{save_path}/{model_name}.pkl\"\n",
    "        joblib.dump(model, save_file)\n",
    "        print(f\"Modèle '{model_name}' sauvegardé sous : {save_file}\")\n",
    "\n",
    "    # Résultats finaux\n",
    "    final_metrics = {\n",
    "        \"Optimal Threshold\": best_threshold,\n",
    "        \"Optimal Business Cost\": best_cost,\n",
    "        \"Results by Threshold\": results_by_threshold\n",
    "    }\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "# Mise à l'échelle des données\n",
    "scaler = StandardScaler()\n",
    "X_train_no_scaled = scaler.fit_transform(X_train)\n",
    "X_test_no_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_bal_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_bal_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Chemin de sauvegarde des modèles\n",
    "save_path = \"/Users/Nelly/Desktop/projet7/best modele/\"\n",
    "\n",
    "# Initialisation des résultats\n",
    "optimized_results_no = {}\n",
    "optimized_results_bal = {}\n",
    "\n",
    "# Évaluation des modèles sans rééquilibrage\n",
    "print(\"Évaluation sur les données SANS rééquilibrage :\")\n",
    "\n",
    "# LightGBM sans rééquilibrage\n",
    "best_model_lgb_no = lgb.LGBMClassifier(\n",
    "    n_estimators=int(best_params_lgb['n_estimators']),\n",
    "    num_leaves=int(best_params_lgb['num_leaves']),\n",
    "    learning_rate=best_params_lgb['learning_rate'],\n",
    "    max_depth=int(best_params_lgb['max_depth']),\n",
    "    subsample=best_params_lgb['subsample'],\n",
    "    colsample_bytree=best_params_lgb['colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"LightGBM\"] = evaluate_optimized_model(\n",
    "    best_model_lgb_no,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"best_model_lgb_no\", #modele optimise LightGBM sans rééquilibrage.\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "# Random Forest sans rééquilibrage\n",
    "best_model_rf_no = RandomForestClassifier(\n",
    "    n_estimators=int(best_params_rf['n_estimators']),\n",
    "    max_depth=int(best_params_rf['max_depth']),\n",
    "    min_samples_split=best_params_rf['min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"Random Forest\"] = evaluate_optimized_model(\n",
    "    best_model_rf_no,\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"best_model_rf_no\", #modele optimise random forest sans rééquilibrage.\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "# Logistic Regression sans rééquilibrage\n",
    "best_model_lr_no = LogisticRegression(\n",
    "    C=best_params_lr['C'],\n",
    "    solver=['liblinear', 'lbfgs'][best_params_lr['solver']],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_no[\"Logistic Regression\"] = evaluate_optimized_model(\n",
    "    best_model_lr_no,\n",
    "    X_train_no_scaled,\n",
    "    X_test_no_scaled,\n",
    "    y_train,\n",
    "    y_test,\n",
    "    model_name=\"best_model_lr_no\", #modele optimise regression linéaire sans rééquilibrage.\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "# Évaluation des modèles avec rééquilibrage\n",
    "print(\"\\nÉvaluation sur les données AVEC rééquilibrage :\")\n",
    "\n",
    "# LightGBM avec rééquilibrage\n",
    "best_model_lgb_bal = lgb.LGBMClassifier(\n",
    "    n_estimators=int(best_params_lgb['n_estimators']),\n",
    "    num_leaves=int(best_params_lgb['num_leaves']),\n",
    "    learning_rate=best_params_lgb['learning_rate'],\n",
    "    max_depth=int(best_params_lgb['max_depth']),\n",
    "    subsample=best_params_lgb['subsample'],\n",
    "    colsample_bytree=best_params_lgb['colsample_bytree'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"LightGBM\"] = evaluate_optimized_model(\n",
    "    best_model_lgb_bal,\n",
    "    X_train_resampled,\n",
    "    X_test,\n",
    "    y_train_resampled,\n",
    "    y_test,\n",
    "    model_name=\"best_model_lgb_bal\",#modele optimise LightGBM avec rééquilibrage.\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "# Random Forest avec rééquilibrage\n",
    "best_model_rf_bal = RandomForestClassifier(\n",
    "    n_estimators=int(best_params_rf['n_estimators']),\n",
    "    max_depth=int(best_params_rf['max_depth']),\n",
    "    min_samples_split=best_params_rf['min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf['min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"Random Forest\"] = evaluate_optimized_model(\n",
    "    best_model_rf_bal,\n",
    "    X_train_resampled,\n",
    "    X_test,\n",
    "    y_train_resampled,\n",
    "    y_test,\n",
    "    model_name=\"best_model_rf_bal\", #modele optimise random forest avec rééquilibrage.\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "# Logistic Regression avec rééquilibrage\n",
    "best_model_lr_bal = LogisticRegression(\n",
    "    C=best_params_lr['C'],\n",
    "    solver=['liblinear', 'lbfgs'][best_params_lr['solver']],\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "optimized_results_bal[\"Logistic Regression\"] = evaluate_optimized_model(\n",
    "    best_model_lr_bal,\n",
    "    X_train_bal_scaled,\n",
    "    X_test_bal_scaled,\n",
    "    y_train_resampled,\n",
    "    y_test,\n",
    "    model_name=\"best_model_lr_bal\", #modele optimise regression linéaire avec rééquilibrage.\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d33b71-913f-40de-9fd4-30e2e36620cf",
   "metadata": {},
   "source": [
    "# <a name=\"C21\"><span style=\"text-decoration: underline;\">Résultat des modèles</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e14519-66f8-425a-9a9a-b1520c7f321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results, title):\n",
    "    \"\"\"\n",
    "    Affichage des résultats de chaque modeles\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    summary_results = []\n",
    "\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\nModèle : {model_name}\")\n",
    "        print(f\"Seuil Optimal : {metrics['Optimal Threshold']:.2f}\")\n",
    "        print(f\"Coût Métier Optimal : {metrics['Optimal Business Cost']:.2f}\\n\")  # Ajout d'un espace ici\n",
    "        \n",
    "        # Résumé au seuil optimal\n",
    "        optimal_metrics = next((res for res in metrics[\"Results by Threshold\"] \n",
    "                                if res[\"Threshold\"] == metrics[\"Optimal Threshold\"]), None)\n",
    "        if optimal_metrics:\n",
    "            summary_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"AUC\": optimal_metrics[\"AUC\"],\n",
    "                \"Accuracy\": optimal_metrics[\"Accuracy\"],\n",
    "                \"Precision\": optimal_metrics[\"Precision\"],\n",
    "                \"Recall\": optimal_metrics[\"Recall\"],\n",
    "                \"F1-Score\": optimal_metrics[\"F1-Score\"],\n",
    "                \"Business Cost\": optimal_metrics[\"Business Cost\"],\n",
    "                \"Optimal Threshold\": optimal_metrics[\"Threshold\"]\n",
    "            })\n",
    "    \n",
    "    # Afficher les résultats sous forme de tableau\n",
    "    results_df = pd.DataFrame(summary_results)\n",
    "    print(results_df.to_string(index=False))  # Le tableau est affiché ici avec un espace au-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fcf8f-74ec-487e-8422-5030958e30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les résultats du DummyClassifier SANS rééquilibrage\n",
    "dummy_results_no = {\n",
    "    \"DummyClassifier No Balancing\": {\n",
    "        \"Optimal Threshold\": optimal_threshold_no,\n",
    "        \"Optimal Business Cost\": dummy_cost_no,\n",
    "        \"Results by Threshold\": [{\n",
    "            \"Threshold\": optimal_threshold_no,\n",
    "            \"AUC\": dummy_auc_no,\n",
    "            \"Accuracy\": dummy_accuracy_no,\n",
    "            \"Precision\": dummy_precision_no,\n",
    "            \"Recall\": dummy_recall_no,\n",
    "            \"F1-Score\": dummy_f1_no,\n",
    "            \"Business Cost\": dummy_cost_no\n",
    "        }]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Préparer les résultats du DummyClassifier AVEC rééquilibrage\n",
    "dummy_results_bal = {\n",
    "    \"DummyClassifier Balancing\": {\n",
    "        \"Optimal Threshold\": optimal_threshold_bal,\n",
    "        \"Optimal Business Cost\": dummy_cost_bal,\n",
    "        \"Results by Threshold\": [{\n",
    "            \"Threshold\": optimal_threshold_bal,\n",
    "            \"AUC\": dummy_auc_bal,\n",
    "            \"Accuracy\": dummy_accuracy_bal,\n",
    "            \"Precision\": dummy_precision_bal,\n",
    "            \"Recall\": dummy_recall_bal,\n",
    "            \"F1-Score\": dummy_f1_bal,\n",
    "            \"Business Cost\": dummy_cost_bal\n",
    "        }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc487b4-5cc4-4b65-a306-ce2f965dbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les résultats du DummyClassifier avec les résultats des modèles optimisés\n",
    "all_results_no = {**optimized_results_no, **dummy_results_no}\n",
    "all_results_bal = {**optimized_results_bal, **dummy_results_bal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a3815a-c7c1-48d0-b9fe-9f2671aa35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats pour SANS rééquilibrage\n",
    "display_results(all_results_no, \"Résultats pour les modèles optimisés SANS rééquilibrage :\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Afficher les résultats pour AVEC rééquilibrage\n",
    "display_results(all_results_bal, \"Résultats pour les modèles optimisés AVEC rééquilibrage :\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18a0f0-f7e5-4411-ade3-6982242c923c",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 4 : Analyse des features du meilleur modèle </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2cff1-0ddb-4268-a68b-abec2d2872f3",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-weight:bold;\">Partie 5 : Data drift </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42371ec-019e-4cc4-933d-332f7dc3185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets\n",
    "df_preprocessed = application_train_test(nan_as_category=True)\n",
    "\n",
    "# Séparer les données d'entraînement (TARGET non nul)\n",
    "train_df = df_preprocessed[df_preprocessed['TARGET'].notnull()]\n",
    "\n",
    "# Charger les données de test (TARGET nul)\n",
    "test_df = df_preprocessed[df_preprocessed['TARGET'].isnull()]\n",
    "\n",
    "# Charger le modèle LightGBM au format .pkl\n",
    "model_file = \"/Users/Nelly/Desktop/projet7/best modele/best_model_lgb_bal.pkl\"\n",
    "if not os.path.exists(model_file):\n",
    "    raise FileNotFoundError(f\"Le fichier de modèle '{model_file}' est introuvable.\")\n",
    "\n",
    "print(\"Chargement du modèle LightGBM sauvegardé en .pkl...\")\n",
    "clf = joblib.load(model_file)  # Charger le modèle avec joblib\n",
    "\n",
    "# Extraire les features du modèle\n",
    "model_features = clf.booster_.feature_name()\n",
    "\n",
    "# Vérifier que toutes les features du modèle sont présentes dans les deux datasets\n",
    "valid_features = [f for f in model_features if f in train_df.columns and f in test_df.columns]\n",
    "\n",
    "# Filtrer les datasets pour inclure uniquement les features pertinentes\n",
    "train_df_filtered = train_df[valid_features]\n",
    "test_df_filtered = test_df[valid_features]\n",
    "\n",
    "# Création d'un rapport Evidently pour le Data Drift sur toutes les features\n",
    "data_drift_report = Report(metrics=[DataDriftPreset()])\n",
    "\n",
    "# Calculer le Data Drift entre les données d'entraînement et de test\n",
    "print(\"Calcul du Data Drift sur toutes les features...\")\n",
    "data_drift_report.run(reference_data=train_df_filtered, current_data=test_df_filtered)\n",
    "\n",
    "# Sauvegarder le rapport HTML\n",
    "output_file = \"data_drift_report_all_features.html\"\n",
    "data_drift_report.save_html(output_file)\n",
    "\n",
    "print(f\"Le rapport de Data Drift a été généré : {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
